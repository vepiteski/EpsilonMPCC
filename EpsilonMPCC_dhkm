% DÃÂÃÂ©claration des packages
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{color}
\usepackage{mathrsfs}
% pour faire des commentaires multi-lignes
\usepackage{comment}
\usepackage{scrextend}
\usepackage{hyperref}
%pour les algos :
%pour l'algorithme
\usepackage{algorithmic}
\usepackage[boxed,linesnumbered,lined,noend]{algorithm2e}
% pour les tableaux
\usepackage{array}
%pour utiliser captionof
\usepackage{caption}
%pour avoir en page large :
\usepackage{fullpage}
% matos pour ajouter des notes
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[normalem]{ulem}
\newcommand{\revjpd}[1]{{\color{blue}#1}}
\newcommand{\revmh}[1]{{\color{red}#1}}
\newcommand{\revak}[1]{{\color{purple}#1}}
\newcommand{\revtm}[1]{{\color{orange}#1}}

\definecolor{notecolor}{gray}{0.55}
\newcommand{\note}{\textcolor{notecolor}}
% a dÃ©commenter quand on veut faire disparaitre les notes :
%\newcommand{\note}[2][]{}


% DÃÂÃÂ©claration des prototypes
\newtheorem{theo}{Theorem}[section]
\newtheorem{lemme}{Lemma}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{defi}{Definition}[section]
\newtheorem{coro}{Corollary}[section]
\newtheorem{enon}{\'Enonc\'e}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{exemple}{Example}[section]

%Déclaration des commandes persos :
\newcommand{\supp}{\mbox{supp}}
\newcommand{\nvar}{n}
\newcommand{\nequa}{m}
\newcommand{\nineq}{p}
\newcommand{\ncomp}{q}

\newcommand{\bbr}{\mathbb{R}}
\newcommand{\bbrn}{\mathbb{R}^\nvar}
\newcommand{\bbrm}{\mathbb{R}^\neq}
\newcommand{\bbrp}{\mathbb{R}^\nineq}
\newcommand{\bbrq}{\mathbb{R}^\ncomp}
\newcommand{\bbn}{\mathbb{N}}

%Commandes pour section 2.1 : non-linear programming
\newcommand{\Ig}{\mathcal{I}_g}
\newcommand{\T}{\mathcal{T}} % le cÃŽne tangent
\newcommand{\Lin}{\mathscr{L}} % le cÃŽne linÃ©arisÃ©
\newcommand{\Crit}{\mathcal{C}} % le cÃŽne critique
\newcommand{\Lag}{\mathcal{L}} % Lagrangien
\newcommand{\feasnlp}{\mathcal{F}} % Feasible set NLP

\newcommand{\M}{\mathcal{M}} %ensemble des multiplicateurs
\newcommand{\m}{\lambda} % multiplicateur complet
\newcommand{\mg}{\lambda^g}\newcommand{\mgk}{\lambda^{g,k}}\newcommand{\mgs}{\lambda^{g,*}} %Multiplicateur g(x)<=0
\newcommand{\mh}{\lambda^h}\newcommand{\mhk}{\lambda^{h,k}}\newcommand{\mhs}{\lambda^{h,*}} %Multiplicateur h(x)=0

%Commandes pour la section 2.2 : MPCC
%ensembles d'indices
\newcommand{\Ioo}{\mathcal{I}^{00}} %G(x)=H(x)=0
\newcommand{\Ipo}{\mathcal{I}^{+0}} %G(x)>0,H(x)=0
\newcommand{\Iop}{\mathcal{I}^{0+}} %G(x)=0,H(x)>0

\newcommand{\LinMPCC}{\Lin_{MPCC}}% le cÃŽne linÃ©arisÃ© MPCC
\newcommand{\LagMPCC}{\mathcal{L}_{MPCC}} % le lagrangien MPCC
\newcommand{\mG}{\lambda^G}\newcommand{\mGk}{\lambda^{G,k}}\newcommand{\mGs}{\lambda^{G,*}} % multiplicateur pour G(x)>=0
\newcommand{\mH}{\lambda^H}\newcommand{\mHk}{\lambda^{H,k}}\newcommand{\mHs}{\lambda^{H,*}} % multiplicateur pour H(x)>=0
\newcommand{\feasmpcc}{\mathcal{Z}} % domaine réalisable MPCC

%Commandes pour la section 3
\newcommand{\LagR}{\mathcal{L}_{R_t}} % Lagrangien
\newcommand{\mP}{\lambda^\Phi}\newcommand{\mPk}{\lambda^{\Phi,k}}\newcommand{\mPs}{\lambda^{\Phi,*}} % multiplicateur de \Phi<=0
%general relaxed nlp :
\newcommand{\nlp}{R_{t_k,r_k}}\newcommand{\nlph}{R_{\hat{t}_k}}
%feasible set butterfly relaxation
\newcommand{\feasb}{\mathcal{X}_{\hat{t}}}
% GÃ©nÃ©rique NLP :
\newcommand{\relaxed}{\Phi(x;t)}\newcommand{\relaxedindice}{\Phi_i(x;t)}\newcommand{\relaxeds}{\Phi(x^*;t)}\newcommand{\relaxedk}{\Phi(x^k;t_k)}\newcommand{\relaxedi}{\Phi_i(x^k;t)}\newcommand{\relaxedki}{\Phi_i(x^k;t_k)}
\newcommand{\relaxedslack}{\Phi(\sg,\sh;t)}\newcommand{\relaxedslackk}{\Phi(\sg^k,\sh^k;t_k)}\newcommand{\relaxedslackki}{\Phi_i(\sg^k,\sh^k;t_k)}\newcommand{\relaxedslacki}{\Phi_i(\sg,\sh;t)}
\newcommand{\FG}{{F_G}} % FG
\newcommand{\FH}{{F_H}} % FH

%Commandes pour la section 7, formulation slack :
\newcommand{\s}{{s}}\newcommand{\si}{s_i} % variable d'ecart pour la complementarite
\newcommand{\sh}{{s_H}}\newcommand{\shi}{{s_{H,i}}}
\newcommand{\sg}{{s_G}}\newcommand{\sgi}{{s_{G,i}}}

\newcommand{\msg}{\lambda^\sg}\newcommand{\msgk}{\lambda^{\sg,k}}\newcommand{\msgs}{\lambda^{\sg,*}} % multiplicateur pour sg(x)>=0
\newcommand{\msh}{\lambda^\sh}\newcommand{\mshk}{\lambda^{\sh,k}}\newcommand{\mshs}{\lambda^{\sh,*}} % multiplicateur pour sh(x)>=0

\newcommand{\y}{{y}}\newcommand{\yi}{y_i} % variable d'ecart pour g(x)<=0

\newcommand{\IG}{\mathcal{I}_G}
\newcommand{\IH}{\mathcal{I}_H}
\newcommand{\IGH}{\mathcal{I}_{GH}}
\newcommand{\IGHOO}{\mathcal{I}_{GH}^{00}}
\newcommand{\IGHOP}{\mathcal{I}_{GH}^{0+}}
\newcommand{\IGHPO}{\mathcal{I}_{GH}^{+0}}
\newcommand{\IGHPP}{\mathcal{I}_{GH}^{++}}

\begin{document}
	
	\title{How to compute local minimum of the MPCC}
	\author{Dussault, J.-P.\footnote{BISOUS} \and Haddou, M\footnote{IRMAR-Insa} \and Kadrani, A\footnote{INSEA} \and Migot, T }
	%\date{}
	\maketitle
	
\begin{abstract}
	We discuss here the convergence of relaxation methods for MPCC with realistic sequence of $\epsilon$-stationary points. We show that by defining a new notion of strong $\epsilon$-stationarity we can attain the goal to compute an M-stationary point.\\
	
	\revjpd{R\'evision Jean-Pierre}
	\revmh{R\'evision Mounir}
	\revak{R\'evision Abdeslam}
	\revtm{R\'evision Tangi}\todo{Autre forme de note}
	
	\textbf{Keywords} : nonlinear programming - MPCC - MPEC - relaxation methods - stationarity - constraint qualification - complementarity \\
	
	\textbf{AMS Subject Classification :} 90C30, 90C33, 49M37, 65K05
\end{abstract}
	
\section{Introduction}
We consider the Mathematical Program with Complementarity Constraint
	\begin{equation}\label{eq:mpcc}
	\begin{split}
	\min\limits_{x \in \bbrn} & f(x) \\
	\mbox{s.t. } & h(x)=0, \ g(x) \leq 0, \\
	& 0 \leq G(x) \perp H(x) \geq 0, 
	\end{split} ~,
	\tag{MPCC}
	\end{equation}
	with $f:\mathbb{R}^n\rightarrow \mathbb{R}$, $h:\mathbb{R}^n \rightarrow \mathbb{R}^m$, $g:\mathbb{R}^n \rightarrow \mathbb{R}^p$ and $G,H:\mathbb{R}^n \rightarrow \mathbb{R}^q$. The notation $0 \leq u \perp v \geq 0$ for two vectors $u$ and $v$ in $\bbrn$ is a shortcut for $u \geq 0, \ v\geq0$ and $u^Tv=0$. In this context solving the problem means finding a local minimum. Even so this goal apparently modest is hard to achieve in general.\\
	
	The discussion presented in this paper is a continuation of the one initiated in \cite{kanzow2015}.\\
	
	Contributions :
	\begin{enumerate}
		\item A generalized framework UFO for relaxation methods.
		\item A new definition of approximate stationary point, which is sufficient to have convergence to M-stationary points.
		\item An algorithm to compute such a stationary point that is a generalization of the penalized-active-set method proposed in \cite{kadrani2015globally}.
	\end{enumerate}

\tableofcontents

\paragraph{Notations }

%Landau asymptotique notation :
We use classical asymptotic Landau notations :

$f(x)=o(g(x))$ as $x \rightarrow a$ if and only if for all positive constant $M$ there exists a positive number $\delta$ such that $| f(x) | \leq M |g(x)|$ for all $|x-a| \leq \delta$, in other words $\lim_{x \rightarrow a} \frac{f(x)}{g(x)}=0$.

$f(x)\sim(g(x))$ as $x \rightarrow a$ if and only $\lim_{x \rightarrow a} \frac{f(x)}{g(x)}=K$ with $K$ a positive finite constant. \\
%We may omit the $(x,\hat{t})$ when the sets of indices are considered in $(x^*,0)$.

\section{Preliminaries}

We give classical definitions from non-linear programming and then present their enhanced version to MPCC that will be used in the sequel.

\subsection{Non-Linear Programming}\label{subsect:nlp}

Let a general non-linear program be
\begin{equation}\label{eq:nlp}
\begin{split}
\min\limits_{x \in \bbr} & f(x) \\
\mbox{s.t. } & h(x)=0, \ g(x) \leq 0
\end{split} ~,
\tag{NLP}
\end{equation}
with $h : \bbrn \rightarrow \bbrm$, $g : \bbrn \rightarrow \bbrp$ and $f : \bbrn \rightarrow \bbr$. Denote $\feasnlp$ the feasible region of $\eqref{eq:nlp}$, the set of active indices $\Ig(x):=\{ i \in \{1,...,\nineq \} \ | \ g_i(x)=0 \}$, the generalized Lagrangian $\Lag^r(x,\m)=rf(x)+g(x)^T\mg+h(x)^T\mh$ where $\m=(\mg,\mh)$ and $\M^r(x)$ is the set of index $r$ multipliers.

By definition, $\m$ is an index $r$ multiplier for \eqref{eq:nlp} at a feasible point $x$ if $(r,\m)\neq 0$ and $\nabla_x \Lag^r(x,\m)=0, \ \m \geq 0, \ g(x)^T\m=0$. An index $0$ multiplier is also called singular multiplier, \cite{bonnans2013perturbation}, or an abnormal multiplier, \cite{clarke1990optimization}. We call a KKT-point a couple $(x,\m)$ with $\m$ an 1-index multiplier at $x$. A couple $(x,\m)$ with $\m$ a 0-index multiplier at $x$ is called Fritz-John point.

In the context of solving non-linear program, that is finding a local minimum, one widely used technique is to compute necessary conditions also called optimality conditions. The principle tool is the Karush-Kuhn-Tucker (KKT) conditions. Let $x^*$ be a local minimum of \eqref{eq:nlp} that satisfy a constraint qualification, then there exists $\m^* \in \M^1(x^*)$ that satisfy
\begin{equation}\label{eq:kkt}
\begin{split}
& \nabla_x \Lag^1(x^*,\m^*)=0, \\
& h(x^*)=0, \ \min( -g(x^*),\mgs)=0
\end{split}
\tag{KKT}
\end{equation}
A point $(x^*,\m^*)$ that satisfy \eqref{eq:kkt} is called a stationary point.

In the context of numerical computation it can be difficult to compute stationary points. Hence, it is of interest to consider $\epsilon$-stationary points.

\begin{defi}[$\epsilon$-stationary point]\label{def:epsilon-stationary-nlp}
	Given a general non-linear program \eqref{eq:nlp} and $\epsilon \geq 0$ with $\epsilon \in \bbr^{\nvar+\nequa+\nineq}$. We say that $(x,\m) \in \bbrn \times \bbr^{\nequa+\nineq}$ is an $\epsilon$-stationary point (or an $\epsilon$-KKT point) if it satisfies
	\begin{equation*}
	\left\| \nabla f(x)+\sum_{i=1}^{\nequa} \lambda^h_i \nabla h_i(x)+\sum_{i=1}^{\nineq} \mg_i \nabla g_i(x) \right\|_\infty \leq \epsilon ~,
	\end{equation*}
	with
	\begin{equation*}
	\begin{split}
	& |h_i(x)|\leq \epsilon, \ \forall i \in \{ 1,\dots,\nequa \} \\
	& g_i(x) \leq \epsilon, \ \mg_i \geq 0, \ \left|\mg_i g_i(x)\right| \leq \epsilon \ \forall i \in \{ 1,\dots,\nineq \}
	\end{split} ~.
	\end{equation*}
\end{defi}

At $\epsilon=0$ we get the classical definition of a stationary point of \eqref{eq:nlp}.

\subsection{Mathematical Program with Complementarity Constraints}\label{subsect:nlpmpcc}

% Section de présentation des CQs, et définition d'optimisation tailored sur les mpcc.

Let $\feasmpcc$ be the set of feasible points of \eqref{eq:mpcc}.
Given $x \in \feasmpcc$, we denote
\begin{eqnarray*}
	\Ipo(x)&:=&\{ i \in \{1,\dots,n\} ~ | ~ G_i(x)>0 \mbox{ and } H_i(x)=0 \} \\
	\Iop(x) &:=& \{ i \in \{1,\dots,n\} ~ | ~ G_i(x)=0 \mbox{ and } H_i(x)>0 \}\\
	\Ioo(x) &:=& \{ i \in \{1,\dots,n\} ~ | ~ G_i(x)=0 \mbox{ and } H_i(x)=0 \}
\end{eqnarray*}

We define the generalized MPCC-Lagrangian function of \eqref{eq:mpcc} as
\begin{equation*}
	\LagMPCC^r(x, \m):= rf(x)+\mg g(x) + \mh h(x) - \mG G(x) - \mH H(x)
\end{equation*}
with $\m:=(\mg, \mh,\mG, \mH)$.
%

We remind that the tangent cone of a set $X$ at $x^* \in X$ is a closed cone defined by
\begin{equation*}
\T_X(x^*)=\{ d \in \bbrn \ | \ \exists t_k \geq 0 \mbox{ and } x^k \rightarrow x^* \mbox{ s.t. } t_k(x^k-x^*) \rightarrow d \} ~.
\end{equation*}
The linearized cone for \eqref{eq:mpcc}, $\LinMPCC$, defined in	 \cite{scheel2000mathematical,flegel2005abadie} is given as the following not necessarily convex cone
\begin{equation*}
\begin{split}
\LinMPCC(x^*):=\{ d \in \bbrn | \nabla h_i(x^*)^Td=0 ~ \forall i=1,\dots,\nequa, \\
\nabla g_i(x^*)^Td\leq0 ~ \forall i=1,\dots,\nineq, \\
\nabla G_i(x^*)^Td=0 ~ \forall i \in \Iop(x^*), \\
\nabla H_i(x^*)^Td=0 ~ \forall i \in \Ipo(x^*),\\
\nabla G_i(x^*)^Td\geq 0, \nabla H_i(x)^Td\geq 0 ~ \forall i \in \Ioo(x^*),\\
(\nabla G_i(x^*)^Td)(\nabla H_i(x^*)^Td)=0 ~ \forall i \in \Ioo(x^*) \}
\end{split} ~.
\end{equation*}
Due to \cite{flegel2005abadie}, one always has the following inclusions
\begin{equation*}
\T_\feasmpcc(x^*) \subseteq \LinMPCC(x^*) ~.
\end{equation*}
Given a cone $K \subset \bbrn$, the polar of $K$ is the cone defined by $K^\circ:=\{ z \in \bbrn \ | \ z^Tx \leq 0, \ \forall x \in K \}$.

We can now define a mild constraint qualification for \eqref{eq:mpcc} called MPCC-Guignard CQ.

\begin{defi}
	Let $x^* \in \mathcal{Z}$. We say that MPCC-GCQ holds at $x^*$ if $\T_\feasmpcc(x^*)^\circ=\LinMPCC(x^*)^\circ$
\end{defi}	

In general, there does not exist KKT stationary points since \eqref{eq:mpcc} is highly degenerate and does not satisfy a constraint qualification in general does not hold. So we introduce weaker stationary concept as in \cite{scheel2000mathematical,jane2005necessary}.

\begin{defi}[Stationary point]
	$x^* \in \feasmpcc$ is said
	\begin{itemize}
		\item Weak-stationary if there exists $\m \in \bbr^{\nequa+\nineq+2\ncomp}$ such that
		\begin{equation*}
		\begin{split}
		&  \nabla_x \LagMPCC^1(x^*,\m)=0, \\
		& \min( -g(x^*),\mg)=0 \\
		& \mG_{\Ipo(x^*)}=0, \ \mH_{\Iop(x^*)}=0
		\end{split}
		\end{equation*}
		\item Clarke-stationary point if $x^*$ is weak-stationary and
		\begin{equation*}
		\forall i \in \Ioo(x^*), \ \mG_i \mH_i \geq 0.
		\end{equation*}
		\item Alternatively(or Abadie)-stationary point if $x^*$ is weak-stationary and
		\begin{equation*}
		\forall i \in \Ioo(x^*), \ \mG_i \geq 0 \mbox{ or } \mH_i \geq 0.
		\end{equation*}
		\item Mordukhovich-stationary point if $x^*$ is weak-stationary and
		\begin{equation*}\label{eq:M_stat_indice_condition}
		\forall i \in \Ioo(x^*), \ \mbox{either } \mG_i>0, ~ \mH_i>0 \mbox{ or } \mG_i \mH_i=0.
		\end{equation*}
		\item Strong-stationary point if $x^*$ is weak-stationary and
		\begin{equation*}
		\forall i \in \Ioo(x^*), \ \mG_i \geq 0, \  \mH_i \geq 0.
		\end{equation*}
	\end{itemize}
\end{defi}
%
Relations between these definitions are straightforward from the definition.

\begin{figure}
	\includegraphics[scale=0.3]{../images/full_stat}
\end{figure}
%
The following theorem is a keystone to define necessary optimality conditions for \eqref{eq:mpcc}.

\begin{theo}[\cite{flegel2006direct}]\label{th:m_stat}
	A local minimum of \eqref{eq:mpcc} that satisfies MPCC-GCQ or any stronger MPCC-CQ is an M-stationary point.
\end{theo}

Therefore, devising algorithms to reach KKT stationary points (S-stationary) is not possible in general, and we must satisfy ourselves in devising algorithms reaching M-stationary points. The following example due to Kanzow and Schwartz exhibits a situation where the global minimizer is not a KKT point but an M-stationary point. We will return to this example later on. 
\begin{exemple}\label{ex:NoKKT}
	\begin{equation}\label{MPCC_Mult1}
	\begin{array}{ll} 
	\min\limits_{x \in \bbr^3} &x_1+x_2-x_3\\
	\textrm{s.t.} & g_1(x):=-4x_1+x_3\le0\\
	& g_2(x):=-4x_2+x_3\le0\\
	& 0\leq G(x):=x_1\perp H(x):=x_2\geq0.\\
	\end{array}
	\end{equation}
	The global solution is $(0,0,0)^t$ but is not a KKT point. Indeed, the gradient of the Lagrangian turns out to be
	\begin{equation}
	\begin{pmatrix}1\\1\\-1\end{pmatrix} +
	\mg_1\begin{pmatrix}-4\\0\\1\end{pmatrix} +
	\mg_2\begin{pmatrix}0\\-4\\1\end{pmatrix} 
	-\mG_1\begin{pmatrix}1\\0\\0\end{pmatrix} 
	-\mH_2\begin{pmatrix}0\\1\\0\end{pmatrix} +
	\eta\begin{pmatrix}0\\0\\0\end{pmatrix}
	\end{equation}
	and since $\mg_1+\mg_2=1$(third line), summing the first two lines yields $2-4(\mg_1+\mg_2)-\mG_1-\mH_2=0$ and therefore $\mG_1+\mH_2=-2$; both cannot be non-negative.
\end{exemple}

\section{A Unified Framework for Relaxation/Approximation Methods}

In the past decade, several methods have been proposed to compute an M-stationary point of \eqref{eq:mpcc}. The first was the approximation scheme proposed by \cite{kadrani2009new}, which was latter improved as a relaxation by \cite{Kanzow2013}. This relaxation scheme has been generalized recently in \cite{migot2016} to a more general family of relaxation schemes. We proposed in this section a unified framework that embraces those methods and may be used to derive new methods.\\

Consider the following parametric non-linear program $R_t(x,s)$ parametrized by $t$ :
\begin{equation}\label{eq:relaxed_nlp}
\begin{split}
\min_{x \in \bbrn } & f(x) \\
\mbox{s.t. } & g(x) \leq 0, \ h(x)=0 \\
& G(x) \geq -\bar{t}(t), \ H(x) \geq -\bar{t}(t), \ \relaxed \leq 0
\end{split} ~,
\tag{$R_t(x)$}
\end{equation}
with  $\bar{t}:\bbr_+^{\#t}  \rightarrow \bbr_+$ such that $\bar{t}(t) \rightarrow_{t\rightarrow 0} 0$ and the relaxation map $\Phi : \bbrn \rightarrow \bbrq$. In the sequel we skip the dependency in t and denote $\bar{t}$ to simplify the notation. It is to be noted here that $t$ is a vector of size $\#t$ as for instance in \cite{migot2016} where $\#t=2$. The Lagrangian function of \eqref{eq:relaxed_nlp} is defined for $\m \in \bbr^{\nequa+\nineq+3\ncomp}$ as 
\begin{equation*}
\LagR^r(x, \m):= rf(x)+g(x)^T\mg+h(x)^T \mh-G(x)^T\mG-H(x)^T\mH+\relaxed^T\mP ~.
\end{equation*}
%Let $\IPHI$ be the set of active indices for the constraint $\relaxed\leq0$ so that
%\begin{equation*}
%\IPHI(x;t)=\{ i \in \{1,\dots,q\} \ | \ \relaxedindice=0 \} ~.
%\end{equation*}

The definition of a generic relaxation scheme is completed by the following hypothesis :
\begin{itemize}
	\item $\relaxed$ is a continuously differentiable real valued map extended component by component.
	\item $\relaxed$ could also be written as a function of two variables $G(x)$ and $H(x)$. Therefore, direct computation gives that the gradient for $i \in \{1,\dots,q\}$ of $\relaxedindice$ for all $x \in \bbrn$ is given by
	\begin{equation*}
	\nabla \relaxedindice:=\nabla G_i(x) \alpha^H_i(x;t) + \nabla H_i(x)  \alpha^G_i(x;t) ~,
	\end{equation*}
	where $\alpha^H(x;t)$ and $\alpha^G(x;t)$ are continuous maps by smoothness assumption on $\relaxed$, which we assume satisfy $\forall x \in \feasmpcc$
	\begin{equation}\label{eq:alpha_tend_vers_G_H}
	\alpha^H(x;t) \rightarrow_{t \rightarrow 0} H(x) \mbox{ and } \alpha^G(x;t) \rightarrow_{t \rightarrow 0} G(x) ~.
	\tag{H2}
	\end{equation}
	\item At the limit when $t$ goes to 0, the feasible set of the parametric non-linear program \eqref{eq:relaxed_nlp} must converges to the feasible set of \eqref{eq:mpcc}. In other words, given $\feasnlp(t)$ the feasible set of \eqref{eq:relaxed_nlp} it holds that
	\begin{equation*}\label{eq:relaxation_converge}
	\lim\limits_{t \downarrow 0} \feasnlp(t) = \feasmpcc ~,
	\tag{H3}
	\end{equation*}
	where the limit is assume pointwise.
	\item At the boundary of the feasible set of the relaxation of the complementarity constraint it holds that
	\begin{equation}\label{eq:frontiere_de_la_relaxation}
	\relaxed=0 \Longleftrightarrow \FG(x;t)=0 \mbox{ or } \FH(x;t)=0
	\tag{H4}
	\end{equation}
	where
	\begin{equation}\label{eq:def_FG_FH_phi}
	\begin{split}
	\FG(x;t)=G(x)-\psi(H(x);t) \\
	\FH(x;t)=H(x) - \psi(G(x);t)
	\end{split} ~,
	\end{equation}
	and $\psi$ is a continuously differentiable real valued function extended component by component. Note that the function $\psi$ may be two different functions in \eqref{eq:def_FG_FH_phi} as long as they satisfy the assumptions below. Those functions $\psi(H(x);t)$,$\psi(G(x);t)$ are non-negative for all $x \in \{ x \in \bbrn \ | \ \relaxed=0 \}$ and satisfy
	\begin{equation}\label{eq:psi_tend_vers_0}
	\lim_{t \downarrow 0} \psi(y,t)=0 \ \forall y \in \bbrq ~.
	\tag{H5}
	\end{equation}
\end{itemize}

We prove in Lemma \ref{le:convergence_M_stationarity} that this generic relaxation scheme for \eqref{eq:mpcc} converges to an M-stationary point requiring the following essential assumption on the functions $\psi$. Given $x \in \bbrn$, then as $t$ goes to 0 the derivative of $\psi$ satisfies for all $i \in \{1,\dots,q\}$
\begin{equation}\label{eq:assumption_1}
	\lim_{t \rightarrow 0} \psi'(G_i(x);t)=\lim_{t \rightarrow 0} \psi'(H_i(x);t)=0 ~.
	\tag{H6}
\end{equation}

We conclude this section by giving an explicit formula for the relaxation map at the boundary of the feasible set.

\begin{lemme}\label{le:gradient_phi_frontiere}
	Given $\relaxed$ such that for all $x \in \{ x \in \bbrn \ | \ \relaxedindice=0 \}$
	\begin{equation*}
	\relaxed=\FG(x;t)\FH(x;t) ~.
	\end{equation*}
	The gradient for $i \in \{1,\dots,q\}$ of $\relaxedindice$ on $x \in \{ x \in \bbrn \ | \ \relaxedindice=0 \}$ is given by
	\begin{equation*}
	\nabla \relaxedindice:=\nabla G_i(x) \alpha^H_i(x;t) + \nabla H_i(x) \alpha^G_i(x;t) ~,
	\end{equation*}
	with
	\begin{equation*}
	\begin{split}
	& \alpha^G(x;t)=\FG(x;t)- \psi'(H(x);t) \FH(x;t) \\
	& \alpha^H(x;t)=\FH(x;t)- \psi'(G(x);t) \FG(x;t)
	\end{split}~.
	\end{equation*}
\end{lemme} 

\section{Existing Methods under the Unified Framework}

In this section, we illustrate the fact that the existing methods in the literature fall under this unified framework. Indeed, the approximation method from Kadrani et al. \cite{kadrani2009new} as well as the two relaxation methods from Kanzow \& Schwarz \cite{Kanzow2013} and from Dussault,Haddou \& Migot \cite{migot2016} satisfy those hypothesis.
We conclude this section by presenting a new asymetric relaxation method that also belong to our framework.

An optimization method that satisfies all of the 6 hypothesis defined in the previous section is called an UF-method.

\subsection{The Boxes Approximation}

In 2009 Kadrani, Dussault and Bechakroun introduce a method, which enjoys the desired goal to converges to an M-stationary point, see \cite{kadrani2009new}. Their original method consider an approximation of the complementarity constraints as a union of two boxes connected only on one point $(t,t)$, in the following way~:
\begin{equation}\label{eq:relaxation-kdb}
\Phi_i^{KDB}(x;t)=(G_i(x)-t)(H_i(x)-t), \ \forall i \in \{1,\dots,\ncomp\} ~.
\end{equation}
This is not a relaxation but an approximation, since the feasible domain of the relaxed problem does not include the feasible domain of \eqref{eq:mpcc}.

\begin{prop}
	The approximation scheme \eqref{eq:relaxed_nlp} with \eqref{eq:relaxation-kdb} is an UF-method.
\end{prop}
\begin{proof}
	Continuity of the map $\Phi$ as well as \eqref{eq:relaxation_converge} has been proved in \cite{kadrani2009new}.
	
	\eqref{eq:frontiere_de_la_relaxation} is satisfied by construction considering $\psi(x;t)=t$. In this case \eqref{eq:assumption_1} and \eqref{eq:psi_tend_vers_0} are obviously satisfied.
	
	Now, we consider \eqref{eq:alpha_tend_vers_G_H}. Direct computation gives that the gradient of $\Phi$ for all $i \in \{1,\dots,\ncomp\}$ is given by
	\begin{equation*}
	\nabla \Phi^{KDB}_{i}(x;t)=
	\nabla G_i(x) (H_i(x)-t) + \nabla H_i(x) (G_i(x)-t) ~.
	\end{equation*}
	Therefore, $\alpha^G_i$ and $\alpha^H_i$ are given by
	\begin{equation*}
	\begin{split}
	\alpha^H_i=H_i(x)-t\\
	\alpha^G_i=	G_i(x)-t
	\end{split}
	\end{equation*}
	It clearly holds that $\alpha^G_i(x;t)\rightarrow_{t \rightarrow 0} G_i(x)$ and $\alpha^H_i(x;t)\rightarrow_{t \rightarrow 0} H_i(x)$. So, in this case \eqref{eq:alpha_tend_vers_G_H} is satisfied.
	
	This completes the proof that all of the 6 hypothesis are satisfied and so the approximation \eqref{eq:relaxation-kdb} is an UF-method.
\end{proof}

\subsection{The L-shape Relaxation}

The previous method has latter been extended to a relaxation in \cite{schwartz2011mathematical,hoheisel2013theoretical} using a piecewise NCP function by considering
\begin{equation}\label{eq:relaxation-ks}
\Phi^{KS}_{i}(x;t)=\phi(G_i(x)-t,H_i(x)-t), \ \forall i \in \{1,\dots,\ncomp\} ~,
\end{equation}
where $\phi:\mathbb{R}^2 \rightarrow \mathbb{R}$ is a continuously differentiable NCP-function with for instance
\begin{equation*}
\phi(a,b)=\begin{cases}
ab, \ \mbox{if } a+b\geq 0\\
-\frac{1}{2}(a^2+b^2), \ \mbox{if } a+b<0
\end{cases} ~.
\end{equation*} 

\begin{prop}
	The relaxation scheme \eqref{eq:relaxed_nlp} with \eqref{eq:relaxation-ks} is an UF-method.
\end{prop}
\begin{proof}
	Continuity of the map $\Phi$ as well as \eqref{eq:relaxation_converge} has been proved in \cite{Kanzow2013}.
	
	\eqref{eq:frontiere_de_la_relaxation} is satisfied by construction considering $\psi(x;t)=t$. In this case \eqref{eq:assumption_1} and \eqref{eq:psi_tend_vers_0} are obviously satisfied.
	
	Now, we consider \eqref{eq:alpha_tend_vers_G_H}. Direct computation gives that the gradient of $\Phi$ for all $i \in \{1,\dots,q\}$ is given by
	\begin{equation*}
	\nabla \Phi^{KS}_{i}(x;t)=\begin{cases}
	\nabla G_i(x) (H_i(x)-t) + \nabla H_i(x) (G_i(x)-t) \mbox{ if } H_i(x)-t+G_i(x)-t \geq 0 \\
	-\nabla G_i(x) (G_i(x)-t) - \nabla H_i(x) (H_i(x)-t) \mbox{ else}
	\end{cases}~.
	\end{equation*}
	Therefore, $\alpha^G_i$ and $\alpha^H_i$ are given by
	\begin{equation*}
	\begin{split}
	\alpha^H_i=\begin{cases}
	H_i(x)-t \mbox{ if } H_i(x)-t+G_i(x)-t \geq 0\\
	G_i(x)-t \mbox{ otherwise}
	\end{cases} \\
	\alpha^G_i=\begin{cases}
	G_i(x)-t \mbox{ if } H_i(x)-t+G_i(x)-t \geq 0 \\
	H_i(x)-t \mbox{ otherwise}
	\end{cases}
	\end{split}
	\end{equation*}
	In the case $H_i(x)-t+G_i(x)-t \geq 0$ it clearly holds that $\alpha^G_i(x;t)\rightarrow G_i(x)$ and $\alpha^H_i(x;t)\rightarrow H_i(x)$. So, in this case \eqref{eq:alpha_tend_vers_G_H} is satisfied.
	
	In the case $H_i(x)-t+G_i(x)-t < 0$ exactly the opposite holds that is $\alpha^G_i(x;t)\rightarrow H_i(x)$ and $\alpha^H_i(x;t)\rightarrow G_i(x)$. However, it is to be noted that sequences $x^t$ with $x^t \rightarrow_{t \rightarrow 0} x^*$ that belongs to this case satisfy $i \in \Ioo(x^*)$. To sum up, in this case for $x \in \feasmpcc$ then $\alpha^G_i(x;t)\rightarrow H_i(x)=G_i(x)=0$ and $\alpha^H_i(x;t)\rightarrow G_i(x)=H_i(x)=0$. This proves that \eqref{eq:alpha_tend_vers_G_H} holds in this case too and so this hypothesis holds for this relaxation.
	
	This completes the proof that all of the 6 hypothesis are satisfied and so the relaxation \eqref{eq:relaxation-ks} is an UF-method.
\end{proof}

\subsection{The Butterfly Relaxation}

The butterfly family of relaxations deal with two positive parameters $(t,r)$ defined such that for all $i \in \{1,\dots,\ncomp\}$
\begin{equation}\label{eq:relaxation-butterfly}
{\Phi^B}_i(x;t,r):=\phi({F_1}_i(x;t,r)){F_2}_i(x;t,r)) ~,
\end{equation}
with
\begin{equation*}
\begin{split}
& {F_1}_i(x;t,r):=(H_i(x)-t \theta_{r}(G_i(x))) \\
& {F_2}_i(x;t,r):=(G_i(x)-t \theta_{r}(H_i(x)))
\end{split} ~,
\end{equation*}
where $\theta_r:\bbr \rightarrow ]-\infty,1]$ are continuously differentiable non-decreasing concave function with $\theta(0)=0$, and $\lim\limits_{t \rightarrow 0} \theta_t(x)=1 ~ \forall x \in \bbr_{++}$ completed in a smooth way for negative values by considering $\theta_{r}(z<0)=\frac{z\theta'(0)t}{r}$. We assume the following relation between the parameter
\begin{equation*}
t=o(r) \mbox{ and } t=\omega(r^2)
\end{equation*}

\begin{prop}
	The relaxation scheme \eqref{eq:relaxed_nlp} with \eqref{eq:relaxation-butterfly} is an UF-method.
\end{prop}
\begin{proof}
	Continuity of the map $\Phi$ as well as \eqref{eq:relaxation_converge} has been proved in \cite{migot2016}.
	
	\eqref{eq:frontiere_de_la_relaxation} is satisfied by construction considering $\psi(z;t)=t \theta_r(z)$. In this case \eqref{eq:psi_tend_vers_0} and \eqref{eq:assumption_1} are obviously satisfied. The latter being insured by $t=o(r)$.
	
	Now, we consider \eqref{eq:alpha_tend_vers_G_H}. Direct computation gives that the gradient of $\Phi$ for all $i \in \{1,\dots,q\}$ is given by
	\begin{equation*}
	\begin{split}
	\nabla \Phi^B_i(x;t,r)=
	\begin{cases}
	&\left({F_1}_i(x;t,r)-t\theta'_r(G_i(x)){F_2}_i(x;t,r)\right) \nabla G_i(x) \\ &
	+ \left({F_2}_i(x;t,r)-t \theta'_r(H_i(x)){F_1}_i(x;t,r)\right) \nabla H_i(x) \mbox{ if } {F_1}_i(x;t,r)+{F_2}_i(x;t,r) \geq 0 \\
	%
	&\left(t\theta'_r(G_i(x)){F_1}_i(x;t,r)- {F_2}_i(x;t,r)\right) \nabla G_i(x) \\ &
	+ \left(t \theta'_r(H_i(x)){F_2}_i(x;t,r)- {F_1}_i(x;t,r)\right) \nabla H_i(x) \mbox{ if } {F_1}_i(x;t,r)+{F_2}_i(x;t,r) < 0 \\
	\end{cases}
	\end{split}
	\end{equation*}
	Therefore, $\alpha^G_i$ and $\alpha^H_i$ are given by
	\begin{equation*}
	\begin{split}
	\alpha^H_i=\begin{cases}
	{F_1}_i(x;t,r)-t\theta'_r(G_i(x)){F_2}_i(x;t,r) \mbox{ if } 	{F_1}_i(x;t,r)+{F_2}_i(x;t,r) \geq 0\\
	t\theta'_r(G_i(x)){F_1}_i(x;t,r)- {F_2}_i(x;t,r) \mbox{ otherwise}
	\end{cases} \\
	\alpha^G_i=\begin{cases}
	{F_2}_i(x;t,r)-t \theta'_r(H_i(x)){F_1}_i(x;t,r) \mbox{ if } {F_1}_i(x;t,r)+{F_2}_i(x;t,r) \geq 0 \\
	t \theta'_r(H_i(x)){F_2}_i(x;t,r)- {F_1}_i(x;t,r) \mbox{ otherwise}
	\end{cases}
	\end{split}
	\end{equation*}
	In the case ${F_1}_i(x;t,r)+{F_2}_i(x;t,r) \geq 0$ it clearly holds that $\alpha^G_i(x;t)\rightarrow G_i(x)$ and $\alpha^H_i(x;t)\rightarrow H_i(x)$. So, in this case \eqref{eq:alpha_tend_vers_G_H} is satisfied.
	
	In the case ${F_1}_i(x;t,r)+{F_2}_i(x;t,r) < 0$ exactly the opposite holds that is $\alpha^G_i(x;t)\rightarrow H_i(x)$ and $\alpha^H_i(x;t)\rightarrow G_i(x)$. However, it is to be noted that a point $x \in \feasmpcc$ that belongs to this case satisfy $G_i(x)=H_i(x)=0$ for $t$ sufficiently small. Therefore, in this case for $x \in \feasmpcc$ then $\alpha^G_i(x;t)\rightarrow H_i(x)=G_i(x)=0$ and $\alpha^H_i(x;t)\rightarrow G_i(x)=H_i(x)=0$. This proves that \eqref{eq:alpha_tend_vers_G_H} holds in this case too and so this hypothesis holds for this relaxation.
	
	This completes the proof that all of the 6 hypothesis are satisfied and so the relaxation \eqref{eq:relaxation-butterfly} is an UF-method.
\end{proof}

\subsection{An Asymmetric Relaxation}

Up till now we only consider relaxation methods that are symmetric. We can define also asymmetric relaxation methods that respect the hypothesis of our unified framework.

Let $I_G$ and $I_H$ be two sets of indices such that $I_G \cup I_H=\{ 1,\dots,\ncomp \}$ and $I_G \cap I_H=\emptyset$. Then, the relaxation constraint is defined with
\begin{equation}\label{eq:relaxation-asymmetric}
\relaxedindice=\begin{cases}
(G(x)-t)H(x)  \mbox{ for } i \in I_G \\
G(x)(H(x)-t) \mbox{ for } i \in I_H
\end{cases}
\end{equation}

\begin{prop}
	The relaxation scheme \eqref{eq:relaxed_nlp} with \eqref{eq:relaxation-asymmetric} is an UF-method.
\end{prop}
\begin{proof}
	Continuity of the map $\relaxed$ as well as \eqref{eq:relaxation_converge} can be easily deduce from the definition of \eqref{eq:relaxation-asymmetric}.
	
	\eqref{eq:frontiere_de_la_relaxation} is satisfied by construction considering $\psi(z;t)=t \mbox{ or } 0$. In this case \eqref{eq:psi_tend_vers_0} and \eqref{eq:assumption_1} are obviously satisfied.
	
	Now, we consider \eqref{eq:alpha_tend_vers_G_H}. Direct computation gives that the gradient of $\Phi$ for all $i \in \{1,\dots,\ncomp\}$ is given by
	\begin{equation*}
	\nabla \relaxedindice(x)=
	\begin{cases}
	\nabla G_i(x) H_i(x)+\nabla H_i(x) (G_i(x)-t) \mbox{ for } i \in I_G  \\
	\nabla G_i(x) (H_i(x)-t)+\nabla H_i(x) G_i(x) \mbox{ for } i \in I_H
	\end{cases}
	\end{equation*}
	Therefore, $\alpha^G_i$ and $\alpha^H_i$ are given by
	\begin{equation*}
	\begin{split}
	\alpha^H_i=\begin{cases}
	H_i(x)\mbox{ for } i \in I_G  \\
	H_i(x)-t  \mbox{ for } i \in I_H
	\end{cases}\\
	\alpha^G_i=\begin{cases}
	G_i(x)-t\mbox{ for } i \in I_G  \\
	G_i(x)  \mbox{ for } i \in I_H
	\end{cases}\\
	\end{split}
	\end{equation*}
	Clearly in both cases \eqref{eq:alpha_tend_vers_G_H} is satisfied.
	
	This completes the proof that all of the 6 hypothesis are satisfied and so the relaxation \eqref{eq:relaxation-ks} is an UF-method.
\end{proof}

\section{Motivations on $\epsilon$-Solution to the Regularized Subproblems}

We have seen in the previous sections a general framework to define relaxations of \eqref{eq:mpcc}. From an algorithmic point of view, the main idea of relaxation methods to solve \eqref{eq:mpcc} is to compute a sequence of stationary points or more precisely approximate stationary points for each value of a sequence of parameter $\{t_k\}$.

The following definition is a specialization of Definition \ref{def:epsilon-stationary-nlp} for \eqref{eq:relaxed_nlp}. It consists in replacing most ``0'' in \eqref{eq:kkt} by small quantities $\epsilon$.

\begin{defi}\label{def:epsilon-stationary}
	$x^k$ is an $\epsilon_k$-stationary point for \eqref{eq:relaxed_nlp} with $\epsilon_k \geq 0$ if there exists $\m^k \in \bbrm \times \bbrp \times \bbr^{3\ncomp}$ such that
	\begin{equation*}
	\begin{split}
	\left\| \nabla \LagR^1(x^k,\m^k;t_k) \right\|_\infty \leq \epsilon_k
	\end{split}
	\end{equation*}
	and
	\begin{equation*}
	\begin{split}
	& |h(x^k)| \leq \epsilon_k \ \forall i \in \{ 1,\dots,\nequa \} \\
	& g_i(x^k) \leq \epsilon_k, \ \mgk_i \geq 0, \ |g_i(x^k)\mgk_i| \leq \epsilon_k \ \forall i \in \{ 1,\dots,\nineq \} \\
	& G_i(x^k)+\bar{t}_k \geq -\epsilon_k, \ {\mGk}_i \geq 0, \ \left|{\mGk}_i ( G_i(x^k)+\bar{t}_k)\right| \leq \epsilon_k \ \forall i \in \{ 1,\dots,\ncomp \} \\
	&  H_i(x^k)+\bar{t}_k \geq -\epsilon_k, \ {\mHk}_i \geq 0, \ \left|{\mHk}_i ( H_i(x^k)+\bar{t}_k)\right| \leq \epsilon_k \ \forall i \in \{ 1,\dots,\ncomp \} \\
	& \relaxedki \leq \epsilon_k, \ \mPk_i \geq 0, \ \left|\mPk_i \relaxedki\right| \leq \epsilon_k \ \forall i \in \{ 1,\dots,\ncomp \} \\
	\end{split} ~.
	\end{equation*}
\end{defi}

Unfortunately, it has been shown in \cite{kanzow2015} (Theorem 9 and 12) or \cite{migot2016} (Theorem 4.3) for the KDB, L-shape and butterfly relaxations that under this definition, sequences of $\epsilon$-stationary points only converge to weak-stationary point without additional hypothesis. 
%
Our goal to compute an M-stationary point with a realistic method is far from obvious. Indeed, $\epsilon$-stationary points have two main drawbacks considering our goal. The difficulties may come from the approximation of the complementarity condition and the approximate feasibility as shown in Example \ref{ex:mpcc_weak} or from the approximation of the feasibility of the relaxed constraint as illustrated in Example \ref{ex:mpcc_C_1}. In those examples, we consider the scheme \eqref{eq:relaxation-kdb} in order to simplify the presentation, but these observations can be easily generalized to the others methods.\\

Kanzow and Schwartz provide the following example exhibiting convergence to a W-stationary point.

\begin{exemple}\label{ex:mpcc_weak}
	\begin{equation*}
	\begin{array}{ll} 
	\min &x_2-x_1\\
	\textrm{s.t.}
	&0\leq x_1\perp x_2\geq0.\\
	\end{array}
	\end{equation*}
	If we perturb the relation $\mP\Phi(x_1,x_2;t)\leq\epsilon$ (leaving the other conditions $\mP\ge0$, $\Phi(x_1,x_2;t)\le0$), $\eta$ may be positive when the constraint $\Phi(x_1,x_2;t)$ is not active. For the case KDB $\Phi(x_1,x_2;t) = (x_1-t)(x_2-t)=-\epsilon^2$ with $\epsilon=t^2$, the point $x(t,\epsilon)={t-\epsilon\choose t+\epsilon}\geq{0\choose0}$ is $\epsilon$--stationary for small enough $\epsilon$: $\Phi(x_1,x_2;t)\le\epsilon$ and the choice $\mP=\frac1\epsilon$ makes the Lagrangian ${-1\choose1}+\mP\Phi(x_1,x_2;t)$ vanish. $x(t)$ converges to the origin when $t,\epsilon\longrightarrow0$ but the origin is only weakly stationary.
\end{exemple}

Now, if the complementarity constraint is relaxed, but the complementarity condition is guaranteed convergence may occur to C-stationary points as shown in the following example.
\begin{exemple}\label{ex:mpcc_C_1}
	\begin{equation*}
	\begin{array}{ll} 
	\min &\frac12((x_1-1)^2 + (x_2-1)^2)\\
	\textrm{s.t.}
	&0\leq x_1\perp x_2\geq0.\\
	\end{array}
	\end{equation*}
	We specialize the relations in Definition \ref{def:epsilon-stationary} as the following, for $t$ and $\epsilon$ close to 0.
	\begin{equation*}
	\begin{array}{cl}
	&\left\|{x_1-1\choose x_2-1}-\mG{1\choose0} - \mH{0\choose1}+\mP{x_2-t\choose x_1-t}\right\|_\infty\leq\epsilon,\\
	&0\leq\mG, \qquad(x_1+t)\geq0, \qquad\mG(x_1+t)\leq\epsilon, \\
	&0\leq\mH, \qquad(x_2+t)\geq0, \qquad\mH(x_2+t)\leq\epsilon, \\
	&0\leq\mP,\qquad(x_1-t)(x_2-t)\leq \epsilon, \qquad\mP\left[(x_1-t)(x_2-t)-\epsilon\right]\geq0.
	\end{array}
	\end{equation*}
	The points $t+\sqrt{\epsilon}\choose t+\sqrt{\epsilon}$ together with $\mG=\mH=0$ and $\mP=\frac{1-t-\sqrt{\epsilon}}{\sqrt{\epsilon}}\nearrow+\infty$ satisfy the above relations. The limit point when $t, \epsilon\longrightarrow0$ is the origin, which is a C-stationary point with $\mG=\mH=-1$.
\end{exemple}
On this example, the relaxed regularized complementarity constraint is active for any small enough $t,\epsilon>0$; moreover, the relaxed regularized stationary point is a local maximum for $t+2\sqrt{\epsilon}<1$. The origin is a local maximum for the original \eqref{eq:mpcc}.

Another example might help understanding the phenomenon.

\begin{exemple}\label{ex:mpcc_normal}
	\begin{equation*}
	\begin{array}{ll} 
	\min &-\frac12((x_1-1)^2 + (x_2-1)^2)\\
	\textrm{s.t.}
	&0\leq x_1\perp x_2\geq0.\\
	\end{array}
	\end{equation*}
	We again specialize the relations in Definition \ref{def:epsilon-stationary} as the following, for $t$ and $\epsilon$ close to 0.
	\begin{equation*}
	\begin{array}{cl}
	&\left\|{1-x_1\choose 1-x_2}-\mG{1\choose0} - \mH{0\choose1}+\mP{x_2-t\choose x_1-t}\right\|_\infty\leq\epsilon,\\
	&0\leq\mG, \qquad(x_1+t)\geq0, \qquad\mG(x_1+t)\leq\epsilon, \\
	&0\leq\mH, \qquad(x_2+t)\geq0, \qquad\mH(x_2+t)\leq\epsilon, \\
	&0\leq\mP,\qquad(x_1-t)(x_2-t)\leq \epsilon, \qquad\mP\left[(x_1-t)(x_2-t)-\epsilon\right]\geq0.
	\end{array}
	\end{equation*}
	This time, the points $t+\sqrt{\epsilon}\choose t+\sqrt{\epsilon}$ are no more $\epsilon$-stationary but the points $x={1\choose -t}$, $\mH=1+t$ and $x={-t\choose 1}$, $\mG=1+t$ are. Their limits are $1\choose 0$ and $0\choose 1$ which are KKT points for the original MPCC with $\mH=1,\mG=0$ or $\mH=0,\mG=1$. The point ${-t\choose -t}$ with $\mH=1+t,\mG=1+t$ is also stationary, and of course converges to the origin, a local minimizer of the original MPCC.
\end{exemple}
In this example, the limit points are not minimizers for the original MPCC, but satisfy the first order KKT conditions for a minimizer. The second order conditions fails for those limit points. The two examples show limiting solutions of regularized subproblems which are not local minimizers of the original MPCC. The first one fails to satisfy a first order condition while the second one satisfies such a first order condition but not the second order one (it is a maximum on the active set).

The Figure \ref{fig:butterfly-epsilon} gives an intuition that explain the weak convergence in Example \ref{ex:mpcc_C_1} by showing the $\epsilon$-feasible set of the butterfly relaxed complementarity constraint.
\begin{figure}
	\begin{center}
		\includegraphics[scale=0.4]{../images/butterfly-epsilon-t2r}
		\captionof{figure}{Butterfly relaxation with $2t=r$ and a constraint $\Phi^B(x;t,r) \leq \epsilon$.}
		\label{fig:butterfly-epsilon}
	\end{center}
\end{figure}
It can be noticed that this feasible set is very similar to the relaxation from Scheel and Scholtes, \cite{scheel2000mathematical}. Therefore, it is no surprise that we can not expect more than convergence to a C-stationary point in these conditions.

\section{Convergence of $\epsilon$-stationary sequences}

We now address the convergence of sequences of $\epsilon$--stationary points.
This motivates the definition of a new kind of $\epsilon$-stationary point called strong $\epsilon_k$-stationary point, which is more stringent regarding the complementarity constraint.

\begin{defi}\label{def:epsilon-stationary-strong}
	$x^k$ is a strong $\epsilon_k$-stationary point for \eqref{eq:relaxed_nlp} with $\epsilon_k \geq 0$ if there exists $\lambda^k \in \bbrm \times \bbrp \times \bbr^{3\ncomp}$ such that
	\begin{equation*}
	\begin{split}
	\left\| \nabla \LagR^1(x^k,\m^k;t_k) \right\|_\infty \leq \epsilon_k
	\end{split}
	\end{equation*}
	and
	\begin{equation*}
	\begin{split}
	& |h(x^k)| \leq \epsilon_k \ \forall i \in \{ 1,\dots,\nequa \} \\
	& g_i(x^k) \leq \epsilon_k, \ \mgk_i \geq 0, \ |g_i(x^k)\mgk_i| \leq \epsilon_k \ \forall i \in \{ 1,\dots,\nineq \} \\
	& G_i(x^k)+\bar{t}_k \geq -\epsilon_k, \ {\mGk}_i \geq 0, \ \left|{\mGk}_i (G_i(x^k)+\bar{t}_k)\right| \leq \epsilon_k \ \forall i \in \{ 1,\dots,\ncomp \} \\
	& H_i(x^k)+\bar{t}_k \geq -\epsilon_k, \ {\mHk}_i \geq 0, \ \left|{\mHk}_i (H_i(x^k)+\bar{t}_k)\right| \leq \epsilon_k \ \forall i \in \{ 1,\dots,\ncomp \} \\
	& \relaxedki \leq 0, \ \mPk_i \geq 0, \ \left|\mPk_i \relaxedki\right| \leq 0 \ \forall i \in \{ 1,\dots,\ncomp \} \\
	\end{split} ~.
	\end{equation*}
\end{defi}

In this case following a similar proof to the one of \cite{kanzow2015} and \cite{migot2016}, we get an improved result that keep the nice properties of the relaxations without strong assumption on the sequence of $\{ \epsilon_k \}$.

The following lemma shows that a sequence of strong $\epsilon_k$-stationary points converges to a weak-stationary point. This is not a new result since the same has been proved in \cite{kanzow2015} and \cite{migot2016} for a sequence of $\epsilon_k$-stationary points. However, the new definition allows to go further by showing convergence to an M-stationary point.

\begin{lemme}\label{le:convergence_M_stationarity}
	Given $\{t_k\}$ a sequence of parameter and $\{\epsilon_k \}$ a sequence of non-negative parameter such that both sequences decrease to zero as $k \in \bbn$ goes to infinity. Assume that $\epsilon_k=o(\bar{t_k})$.
	Let $\{ x^k,\m^k\}_k$ be a sequence of strong $\epsilon_k$-stationary points of \eqref{eq:relaxed_nlp} according to definition \ref{def:epsilon-stationary-strong} for all $k\in \bbn$ with $x^k \rightarrow x^*$. Let $ \{\eta^{G,k}\}, \{ \eta^{H,k} \} $ be two sequences such that
	\begin{equation}\label{eq:definition_etaG_etaH}
	\begin{split}
	\eta^{G,k}:=\mGk-\mPk\alpha^H(x^k;t_k) \\
	\eta^{H,k}:=\mHk-\mPk\alpha^G(x^k;t_k)
	\end{split} ~.
	\end{equation}
	Assume that the sequence of multipliers $\{\mhk,\mgk,\eta^{G,k},\eta^{H,k} \}$ is bounded.
	Then, $x^*$ is an M-stationary point of \eqref{eq:mpcc}.
\end{lemme}
\begin{proof}
	The proof is divided in two parts. We first show that $x^*$ is a weak-stationary point and then we prove that is an M-stationary point.\\
	
	Let us prove the first part of the lemma.
	By definition $\{ x^k,\m^k\}_k$ is a sequence of strong $\epsilon_k$-stationary points of \eqref{eq:relaxed_nlp}. We make the condition on the Lagrangian
	\begin{equation*}
	\left\| \nabla \LagR^1(x^k,\m^k;t_k) \right\|_\infty \leq \epsilon_k
	\end{equation*}
	more explicit.
	By construction of $\relaxed$ this condition becomes
	\begin{equation}\label{eq:dual_feasibility_epsilon}
	\begin{split}
	\left\| \nabla f(x^k)+\nabla g(x^k)^T\mgk+\nabla h(x^k)^T\mhk -\nabla G(x^k)^T\eta^{G,k} -\nabla H(x^k)^T\eta^{H,k} \right\|_\infty \leq \epsilon_k
	\end{split} ~.
	\end{equation}
	Besides, the sequence of multipliers $\{\mhk,\mgk,\eta^{G,k},\eta^{H,k} \}$ is assumed bounded. Therefore, it follows that the sequence converges to some limit point
	\begin{equation*}
	\{\mhk,\mgk,\eta^{G,k},\eta^{H,k} \} \rightarrow (\mh,\mg,\eta^G,\eta^H) ~.
	\end{equation*}
	It is to be noted that for $k$ sufficiently large it holds
	\begin{equation*}
	\begin{split}
	& \supp(\mgk) \subset \supp(\mg) \\
	& \supp(\eta^{G,k}) \subset \supp(\eta^G) \\
	& \supp(\eta^{H,k}) \subset \supp(\eta^H) \\	
	\end{split} ~.
	\end{equation*}
We prove that $(x^*,\mh,\mg,\eta^G,\eta^H)$ is a weak-stationary point. Obviously, since $\epsilon_k \downarrow 0$ it follows that $x^* \in \feasmpcc$, $\nabla_x \LagMPCC^1(x^*,\mh,\mg,\eta^G,\eta^H)=0$ by \eqref{eq:dual_feasibility_epsilon} and that $\mg_i=0$ for $i \notin \Ig(x^*)$. It remains to show that for indices $i \in \Ipo(x^*)$, $\eta^G_i=0$. The opposite case for indices $i \in \Iop(x^*)$ would follow in a completely similar way. So, let $i$ be in $\Ipo(x^*)$.\\
%
By definition of strong $\epsilon_k$-stationarity it holds for all $k$ that
\begin{equation*}
|\mGk_i(G_i(x^k)+\bar{t_k})| \leq \epsilon_k ~.
\end{equation*}
Therefore, $\mGk_i \rightarrow_{k \rightarrow \infty} 0$ since $\epsilon_k \downarrow 0$ and $G_i(x^k) \rightarrow G_i(x^*) >0$.\\
Without loss of generality we may assume that for $k$ sufficiently large $\mPk_i \neq 0$ otherwise $\eta^G_i=0$ and the proof is complete. By strong $\epsilon$-stationarity $\mPk_i \neq 0$ implies that $F_{H,i}(x^k;t_k)=0$ by \eqref{eq:frontiere_de_la_relaxation}.
%
\eqref{eq:alpha_tend_vers_G_H} yields $\alpha^H_i(x^k;t_k) \rightarrow H(x^*)$ and so $\eta^G_i=0$ unless $\mPk$ diverges as $k$ grows. We now prove that the latter case leads to a contradiction.

Assume that $\mPk \rightarrow \infty$, boundedness hypothesis on $\eta^G_i$ gives that there exists a finite non-vanishing constant $C$ such that 
\begin{equation*}
\mPk_i \alpha^H_i(x^k;t_k) \rightarrow C ~.
\end{equation*}
Moreover, since $\eta^H_i$ is finite and $\mPk_i \alpha^G_i(x^k;t_k) \rightarrow \infty$ as $G_i(x^k)>0$ then necessarily $\mHk_i \rightarrow \infty$. Furthermore, noticing that $F_H(x^k;t_k)=0$ gives $H_i(x^k)\geq0$,  leads to a contradiction with $\mHk_i \rightarrow \infty$ since by $\epsilon$-stationarity we get
\begin{equation*}
\left|\mHk_i (H_i(x^k)+\bar{t}_k)\right|=\left|\mHk_i H_i(x^k)\right| + \left|\mHk_i \bar{t}_k\right|\leq \epsilon_k
\end{equation*}
and $\epsilon_k=o(\bar{t}_k)$.

We can conclude that for $i \in \Ipo(x^*)$, $\eta^G_i=0$ and therefore $x^*$ is a weak-stationary point.\\

	Now, let us prove that $x^*$ is even stronger that weak-stationary point since it is an M-stationary point.
	We now consider indices $i \in \Ioo(x^*)$.	
	Our aim here is to prove that either $\eta^G_i>0,\eta^H_i>0$ either $\eta^G_i \eta^H_i=0$. It is clear that if $\mPk_i=0$, then $\eta^G_i$ and $\eta^H_i$ are non-negative values and the result holds true. So, without loss of generality we may assume that $\mPk_i\geq 0$ and then $\relaxedki =0$ by Definition \ref{def:epsilon-stationary-strong}.
	
	By construction of $\relaxedki$ given in hypothesis \eqref{eq:frontiere_de_la_relaxation} it follows that $\relaxedki=0 \Longleftrightarrow F_{G,i}(x^k;t_k)=0 \mbox{ or } F_{H,i}(x^k;t_k)=0$ where we remind that
	\begin{equation*}
	\begin{split}
	F_{G,i}(x^k;t_k)=G_i(x^k)-\psi(H_i(x^k);t_k) \\
	F_{H,i}(x^k;t_k)=H_i(x^k) - \psi(G_i(x^k);t_k)
	\end{split} ~,
	\end{equation*}
	
	Without loss of generality we assume that $F_{G,i}(x^k;t_k)=0$ since the other case is completely similar. Furthermore by construction of $\relaxedki$ it holds that $G_i(x^k)$ and $H_i(x^k)$ are non-negative in this case.
	
	We prove that $\eta^H_i=0$ in this case in two steps, first we prove that $\mGk_i$ and $\mHk_i$ are converging quickly to zero and second we prove that $\alpha^G_i(x^k;t_k) \mPk_i$ converges to zero.
	
	Considering one of the complementarity condition of the strong $\epsilon$-stationarity gives
	\begin{equation*}
	\epsilon_k \geq |\mGk_i(G_i(x^k)+\bar{t_k})|=|\mGk_iG_i(x^k)|+|\mGk_i\bar{t_k}| ~,
	\end{equation*}
	and it follows that
	\begin{equation*}
	|\mGk_i\bar{t_k}| \leq \epsilon_k
	\end{equation*}
	Necessarily $\mGk_i \rightarrow_{k \rightarrow \infty} 0$ as we assume in our statement that $\epsilon_k=o(\bar{t_k})$. Using the same argument we get $\mHk_i \rightarrow_{k \rightarrow \infty} 0$.
	
	Now at $x^k$ we can use Lemma \ref{le:gradient_phi_frontiere} that for $\FG(x^k;t_k)=0$ gives
	\begin{equation*}
	\begin{split}
	& \alpha^G_i(x^k;t_k)=- \psi'(H_i(x^k);t_k) F_{H,i}(x^k;t_k) \\
	& \alpha^H_i(x^k;t_k)=F_{H,i}(x^k;t_k)
	\end{split}~.
	\end{equation*}
	By hypothesis \eqref{eq:assumption_1}, it holds that $\psi'(H_i(x^k);t_k) \rightarrow_{k \rightarrow \infty} 0$. Therefore, $\alpha^G_i(x^k;t_k) \mPk_i$ going to a non-zero limit would imply that $\alpha^H_i(x^k;t_k) \mPk_i$ goes to infinity. However, this is a contradiction with $\eta^G_i$ being finite. We can conclude that necessarily $\alpha^G_i(x^k;t_k) \mPk_i$ converges to zero.
	
	It follows that $\eta^H=0$ and so $x^*$ is an M-stationary point.
\end{proof}

The following theorem is a direct consequence of both previous lemmas and is our main statement.

\begin{theo}\label{th:convergence_butterfly_epsilon-2}
	Given $\{t_k\}$ a sequence of parameter and $\{\epsilon_k \}$ a sequence of non-negative parameter such that both sequences decrease to zero as $k \in \bbn$ goes to infinity. Assume that $\epsilon_k=o(\bar{t_k})$.
	Let $\{ x^k,\m^k\}_k$ be a sequence of $\epsilon_k$-stationary points of \eqref{eq:relaxed_nlp} according to definition \ref{def:epsilon-stationary-strong} for all $k\in \bbn$ with $x^k \rightarrow x^*$ such that MPCC-CRSC holds at $x^*$.
	Then, $x^*$ is an M-stationary point of \eqref{eq:mpcc}.
\end{theo}
\begin{proof}
	The proof is direct by Lemma \ref{le:convergence_M_stationarity} and Corollary 2.3 of \cite{migot2016} that ensures boundedness of the sequence \eqref{eq:definition_etaG_etaH} under MPCC-CRSC.
\end{proof}

Theorem \eqref{th:convergence_butterfly_epsilon-2} attains the ultimate goal, however it is not a trivial task to compute such a sequence of $\epsilon$-stationary points. This is discussed in the next section.

\section{Lagrange Multipliers of the Regularization}

The following example develops on example \ref{ex:NoKKT} due to Kanzow and Schwartz and exhibits a situation where  the regularized subproblems have no KKT point.
\begin{exemple}\label{ex:NoKKT2}
	The KDB regularized problem is
	\begin{equation}\label{MPCC_Mult2}
	\begin{array}{ll} 
	\min &x_1+x_2-x_3\\
	\textrm{s.t.}&-4x_1+x_3\le0\\
	&-4x_2+x_3\le0\\
	&x_1\geq-t\\
	&x_2\geq-t\\
	&(x_1-t)(x_2-t)\le0.\\
	\end{array}
	\end{equation}
	The point $(t,t,4t)^t$ is feasible so that the minimum value of this program is $\leq -2t$. Moreover, whenever $x_1>t$, we must have $x_2<=t$ to satisfy $(x_1-t)(x_2-t)\le0$. This allows to conclude that $(t,t,4t)^t$ is the global minimum of the regularized problem. $\mG=\mH=0$ and the gradient of the Lagrangian yields
	\begin{equation}
	\begin{pmatrix}1\\1\\-1\end{pmatrix} +
	\mg_1\begin{pmatrix}-4\\0\\1\end{pmatrix} +
	\mg_2\begin{pmatrix}0\\-4\\1\end{pmatrix} +
	\mP\begin{pmatrix}0\\0\\0\end{pmatrix}
	\end{equation}
	which cannot be satisfied.
\end{exemple}
This last example seems to contradict Theorem 4.6 in \cite{Kanzow2013}, but MPCC-LICQ is not satisfied by four constraints in $\mathbb{R}^3$.

It has been pointed out earlier that a practical algorithm may not be able to compute stationary point of the regularized subproblem, but only some approximate $\epsilon$-stationary point. Existance of such points are guaranteed under some weaker condition than MPCC-LICQ. We denote $\feasb$ the feasible set of the regularized subproblem \eqref{eq:relaxed_nlp}.

\begin{theo}\label{th:butterfly_cq_gcq}
	Let $x^* \in \feasb$ such that MPCC-CCP holds at $x^*$. Then, there exists $\bar{t}>0$ and a neighbourhood $U(x^*)$ of $x^*$ such that the following holds for all $t \in (0,\bar{t}]$. If $x^*$ is M-stationary, then there exists $\epsilon$-stationary points at any $x \in U(x^*)$ of $ \nlph$.
\end{theo}
\begin{proof}
	\todo[inline]{Preuve en travaux}
	\begin{itemize}
		\item MPCC-CCP donne que les s\'equences qui vont vers un point M-stat. convergent.
		\item On v\'erifie que ces s\'equences sont des s\'equences de points $\epsilon$-stationaires.
	\end{itemize}
\end{proof}

\todo[inline]{Debut de discussion sur l'existence de points strong $\epsilon$-stationary}
However, the strong $\epsilon$-stationary point suffer from the same difficulties than Example \ref{ex:NoKKT2}.

\begin{theo}
	Let $x^* \in \feasb$ be such that MPCC-LICQ holds at $x^*$. Then, there exists $\bar{t}>0$ and a neighbourhood $U(x^*)$ of $x^*$ such that the following holds for all $t \in (0,\bar{t}]$. If $x \in U(x^*)\cap \feasb$ and $\{ i \ | \ | \FG=\FH=0 \}=\emptyset$, then standard LICQ for $\nlph$ holds in $x$.
\end{theo}

Un exemple pour r\'eflechir sur ce qu'il manque. Dans l'exemple \ref{ex:NoKKT2} on pourrait toujours argumenter que la s\'equence qui passe par le point qui d\'erange converge vers un point KKT.

\begin{exemple}
	Donner un exemple o\`u on a deux contraintes de compl\'ementarit\'e et la s\'equence est $G(x^k)=(t, t)$ $H(x^k)=(t, 2t)$ et le point limite est M-stationaire + pas de MPCC-LICQ.
\end{exemple}
\todo[inline]{Fin de discussion sur l'existence de points strong $\epsilon$-stationary}

\section{How to Compute Strong $\epsilon$-Stationary Points}

The previous section introduces the new concept of strong $\epsilon$-stationary point considering the relaxed sub-problems. In this section, we answer the non-trivial question of how to compute such an approximate stationary point. We present here a generalization of the penalization with active set scheme proposed in \cite{kadrani2015globally} and illustrate the fact that it has the desired property.

\subsection{Slack Formulation}

Consider the following parametric non-linear program $R_t(x,\s)$ parametrized by $t$ :
\begin{equation}\label{eq:relaxed_nlp_slack}
\begin{split}
\min_{(x,\s) \in \bbrn \times \bbr^{2\ncomp}} & f(x) \\
\mbox{s.t. } & g(x) \leq 0, \ h(x)=0 \\
& \sg=G(x), \ \sh=H(x) \\
& \sg \geq -\bar{t}, \ \sh \geq -\bar{t}, \ \relaxedslack \leq 0
\end{split} ~,
\tag{$R^s_t(x,\s)$}
\end{equation}
with $\bar{t} \downarrow_{t\downarrow 0} 0$ and the relaxation map $\relaxedslack:\bbrq \rightarrow \bbrq$ is defined by replacing $G(x)$ and $H(x)$ by $\sg$ and $\sh$ in the map $\relaxed$.

The Lagrangian function of \eqref{eq:relaxed_nlp_slack} is defined as $\Lag^r_s(x,\s, \lambda;t):= rf(x)+g(x)^T\mg+h(x)^T\mh-(G(x)-\sg)^T\mG-(H(x)-\sh)^T\mH-\sg^T\msg-\sh^T\msh+\Phi(\sg,\sh;t)^T\mP$.

The following result is a direct corollary of Theorem \ref{th:convergence_butterfly_epsilon-2} stating that the reformulation with slack variables does not alter the result.

\begin{coro}\label{th:convergence_butterfly_epsilon-2-slack}
	Given $\{t_k\}$ a sequence of parameter and $\{\epsilon_k \}$ a sequence of non-negative parameter such that both sequences decrease to zero as $k \in \bbn$ goes to infinity. Assume that $\epsilon_k=o(\bar{t_k})$.
	Let $\{ x^k,\m^k\}_k$ be a sequence of $\epsilon_k$-stationary points of \eqref{eq:relaxed_nlp_slack} for all $k\in \bbn$ with $x^k \rightarrow x^*$ such that MPCC-CRSC holds at $x^*$.
	Then, $x^*$ is an M-stationary point of \eqref{eq:mpcc}.
\end{coro}
\begin{proof}
	Let $\tilde{h}(x):\bbrn \rightarrow \bbr^{m+2q}$ be such that $\tilde{h}(x):=(h(x),\sg-G(x),\sh-H(x))^T$ and $\tilde{x}:=(x,\sg,\sh)^T$. It is clear that the non-linear program \eqref{eq:relaxed_nlp_slack} fall under the formulation \eqref{eq:relaxed_nlp}. Therefore, we can apply \ref{th:convergence_butterfly_epsilon-2} to conclude this proof.
\end{proof}

The following lemmas giving an explicit form of the gradient of the Lagrangian function of \eqref{eq:relaxed_nlp_slack} can be deduce by direct computation.

\begin{lemme}
	The gradient of $\Lag^r_s(x,\s, \lambda;t)$ is given by
	\begin{equation}
	\nabla_x \Lag^r_s(x,\s, \lambda;t)=r\nabla f(x) + {\mg}^T \nabla g(x) + {\mh}^T \nabla h(x) - {\mG}^T\nabla G(x) - {\mH}^T \nabla H(x) ~,
	\end{equation}
	\begin{equation}
	\begin{split}
	\nabla_{\sg} \Lag^r_s(x,\s, \lambda;t)=\mG-\lambda^{\sg}+\mP \nabla_{\sg} \relaxedslack \\
	\nabla_{\sh} \Lag^r_s(x,\s, \lambda;t)=\mH-\lambda^{\sh}+\mP \nabla_{\sh} \relaxedslack
	\end{split} ~.
	\end{equation}
\end{lemme}

There is two direct consequences of this result. Firstly, it is easy to see from this Lemma that computing a stationary point of $\Lag^r_s(x,s, \lambda;t)$ is equivalent to computing a stationary point of $\Lag^r(x,s, \lambda;t)$. Secondly, a stationary point of $\Lag^r_s(x,s, \lambda;t)$ with $r=1$ satisfies one of the condition of weak-stationary point of \eqref{eq:mpcc} that is $\Lag^1_{MPCC}(x,\lambda)=0$.\\

\subsection{A Penalization Formulation}

The following minimization problem aims at finding the quadruple $(x,\s,\y,t) \in \bbrn \times \bbr^{2\ncomp} \times \bbrp \times \bbr^{\#t}$ so that

\begin{equation*}\label{eq:penalized_nlp}
\begin{split}
\min &\Psi_{r,\rho}(x,\s,\y,t):= f(x)+ \frac{1}{2r}\phi(x,\s,\y)+\rho t\\
\mbox{s.t. } &  \y \geq 0 \\
& \sg \geq -\bar{t}, \ \sh \geq -\bar{t}, \ \relaxed \leq 0
\end{split} ~,
\tag{$P_t(x,s)$}
\end{equation*}
where $\phi$ is the penalty function
\begin{equation*}
\phi(x,\s,\y)=\| (g(x)+\y,h(x),G(x)-\sg,H(x)-\sh) \|^2 ~.
\end{equation*}

An adaptation of Theorem \ref{th:convergence_butterfly_epsilon-2} gives the following result that validate the penalization approach.

\begin{theo}\label{th:convergence_butterfly_epsilon-pen}
	Given $\{r_k\}$, $\{ \rho_k \}$ decreasing and non-decreasing sequences of parameter and $\{\epsilon_k \}$ a sequence of non-negative parameter that decreases to zero as $k \in \bbn$ goes to infinity. Assume that $\epsilon_k=o(\bar{t_k})$.
	Let $\{ x^k,\m^k\}_k$ be a sequence of $\epsilon_k$-stationary points of \eqref{eq:penalized_nlp} according to definition \ref{def:epsilon-stationary-strong} for all $k\in \bbn$ with $x^k \rightarrow x^*$ such that MPCC-CRSC holds at $x^*$.
	If $x^*$ is feasible, then it is an M-stationary point of \eqref{eq:mpcc}.
\end{theo}
\begin{proof}
	Assuming that $x^*$ is feasible for \eqref{eq:mpcc}, the result would be a straightforward adaptation of Theorem \ref{th:convergence_butterfly_epsilon-2}.
\end{proof}

\begin{remark}
	Unfortunately, the annoying assumption on the previous theorem that $x^*$ must be feasible is hard to avoid. Indeed, it is a classical pitfall in penalization methods in optimization to possibly compute a limit point that minimizes the linear combination of the constraints. In other words, we compute a point $x^*$ infeasible that satisfies
	\begin{equation*}
	\sum_{i=1}^{\nequa} h_i(x^*)\nabla h_i(x^*)+\sum_{i=1}^{\nineq} \max(-g_i(x^*),0)\nabla g_i(x^*)-\sum_{i=1}^{\ncomp} \max(G_i(x^*),0)\nabla G_i(x^*)-\sum_{i=1}^{\ncomp} \max(H_i(x^*),0)\nabla H_i(x^*)=0 ~.
	\end{equation*}
	This phenomenon has been well-known in non-linear programming methods using interior-point or filter for instance. Such a point is sometimes call infeasible stationary point.
\end{remark}

It is interesting to note that the way the penalty parameters $r$ and $\rho$ behave may provide some informations on the quality of the limit point. Indeed, if in some sense we find a stationary point of the initial problem without driving $\rho$ to infinity, then we get an S-stationary point. This observation was introduced in \cite{Coulibaly2012} in the context of elastic interior-point for \eqref{eq:mpcc} and then adapted to the penalization technique from \cite{kadrani2015globally}.

\begin{theo}
	Let $(x,\s,\y,t)$ be a strong $\epsilon$-stationary point of \eqref{eq:penalized_nlp} with $r,\rho>0$. If $x$ is feasible for \eqref{eq:mpcc}, then $t=0$ and $x$ is an S-stationary point of \eqref{eq:mpcc}.
\end{theo}

This fact was already observed in Theorem 2 of \cite{kadrani2015globally} in a slightly weaker but similar framework. We do not repeat the proof, but gives an interpretation of this result.

It has been made clear in the proof of the convergence theorem, Theorem \ref{th:convergence_butterfly_epsilon-2}, that the case where $x^*$ is an M-stationary point only occur if the sequence of multipliers $\{ \mPk \}$ diverges. Therefore, it seems logical that necessary the penalty parameters must be driven to their limit to observe such phenomenon.

\subsection{Active Set Method for \eqref{eq:penalized_nlp}}

\todo[inline]{En cours de r\'edaction}

We now discuss an active set method to solve the penalized problem \eqref{eq:penalized_nlp}. An essential assumption here is the existence of some Lagrange multipliers for the regularized problem. This has been first proposed in \cite{kadrani2015globally}. We extend it here to the general class of methods presented here.

Let $\mathcal{W}(s;t)$ be the set of active constraints among the constraints
\begin{equation}\label{eq:contraintes}
\y \geq 0, \ \sg \geq -\bar{t}, \ \sh \geq -\bar{t}, \ \relaxedslack \leq 0 ~.
\end{equation}
We can be even more specific when the relaxed constraint is active since
\begin{equation*}
\relaxedslack = 0 \Longleftrightarrow \sh=\psi(\sg;t) \mbox{ or } \sg=\psi(\sh;t) ~.
\end{equation*}

\begin{remark}\label{re:substitution}
	It is essential to note here that active constraint act almost like bound constraints since an active constraint means that one (possibly both) of the two case holds
	\begin{equation*}
	\begin{split}
	\sg=& \begin{cases}
	-\bar{t} \mbox{ or } \psi(\sh;t)
	\end{cases} \\
	&\mbox{ or } \\
	\sh=& \begin{cases}
	-\bar{t} \mbox{ or } \psi(\sg;t)
	\end{cases} 
	\end{split} ~.
	\end{equation*}
	Considering the relaxation from Kanzow \& Schwarz it is obviously a bound constraint since $\psi(\sg;t)=\psi(\sh;t)=t$. The butterfly relaxation gives $\psi(\sg;t)=t\theta_r(\sh)$ and $\psi(\sh;t)=t\theta_r(\sg)$. This is not a bound constraint but we can easily use a substitution technique. This key observation is the main motivation to use a formulation with slack variables.
\end{remark}

\begin{algorithm}[H]
	\KwData{ \\Initial data $x^{k-1},\s^{k-1},t_k$; precision $\epsilon>0$ \; Initial estimate of the multiplier $\lambda^0$\; Initial working set $\mathcal{W}_0$ of active constraints, $A_0$ initial matrix of gradients of active constraints.\;sat:=true}
	\textbf{Begin} \;
	Set $j:=0$ \; 
	Projection of $x^{k-1},\s^{k-1},t_k$ if not feasible for \eqref{eq:penalized_nlp} \;
	\While{sat \mbox{\textbf{and}}  $\left(\|\nabla \Lag^1(x^k,\sg^k,\sh^k,\lambda^k;t_k))\|_p^2>\epsilon \mbox{ \textbf{or} } \min(\lambda) < 0\right)$ }{
		\label{step:substitution} Substitution of the variables that are fixed by the active constraints in $\mathcal{W}_{j}$\;
		Compute a feasible direction $d^j$ that lies in the subspace defined by the working set $\mathcal{W}_{j}$ (see \eqref{eq:chain_rule}) and satisfies the conditions \eqref{eq:direction_descente_suffisante}\;
		Determine the approximate multipliers $\m^j=(\mg,\mG,\mH,\mP)$ by solving
		$$ \min_{\m^j \in \bbr^{\mathcal{W_j}}} \| A_j^T \m^j -   \nabla \Psi_{r,\rho}(z^j,t) \|^2 $$
		Relaxing rule : \If{ $\exists \ i, \ \lambda_i^j<0$ + satisfy \eqref{eq:Wolfe_cond}}{Update of the working set $\bar{\mathcal{W}_j}$ (with an anti-cycling rule) \;}
		Compute $\bar{\alpha}$ the maximum non-negative feasible step along $d^j$
		$$
		\bar{\alpha}:=\sup \{ \alpha : z^j+\alpha d^j \in \feasnlp \}
		$$
		Compute a step length $\alpha_j \leq \bar{\alpha}$ such that Armijo condition \eqref{eq:Armijo_cond} holds \; 
		\If{$\alpha_j=\bar{\alpha}$}{Update the working set $\rightarrow \mathcal{W}_{j+1}$ and compute $A_{j+1}$ the matrix of gradients of active constraints}
		
		$(x^{k,j+1},\s^{k,j+1})=(x^{k,j},\s^{k,j})+\alpha_j d^j$ \;
		j:=j+1 \;
		sat:=$\|d^j\| >\epsilon$
	}
	\Return : $x^k,s^k$ or a decision of unboundedness.
	\caption{Outer loop iteration : active set method for relaxed non-linear program \eqref{eq:penalized_nlp}.}
	\label{algo:active_set_mpcc}
\end{algorithm}

At each step, the set $\mathcal{W}_j$ denotes the set of active constraints of the current iterate $x^{k,j}$. As pointed out in Remark \ref{re:substitution} those active constraints fix some of the variables. Therefore, replacing those fixed variables we can rewrite the problem in a subspace as
The following minimization problem aims at finding the quadruple $(x,\s,\y,t) \in \bbrn \times \bbr^{2\ncomp} \times \bbrp \times \bbr^{\#t}$ so that
%
\begin{equation*}
\begin{split}
\min &\tilde{\Psi}_{r,\rho}(x,\s_{\mathcal{S}_G \cup \mathcal{S}_H},\y_\mathcal{Y},t):= f(x)+ \frac{1}{2r}\tilde{\phi}(x,\s_{\mathcal{S}_G \cup \mathcal{S}_H},\y_\mathcal{Y})+\rho t\\
\mbox{s.t. } &  \yi \geq 0 \mbox{ for } i \in \mathcal{Y}, \\
& \sgi \geq -\bar{t} \mbox{ for } i \in \mathcal{S}_G, \ \shi \geq -\bar{t} \mbox{ for } i \in \mathcal{S}_H, \\
&\relaxedslacki \leq 0 \mbox{ for } i \in \mathcal{S}_G \cup \mathcal{S}_H
\end{split} ~,
\tag{$\tilde{P}_t(x,\s)$}
\end{equation*}
with 
\begin{eqnarray*}
	\mathcal{Y} &:=& \{ i \in \{1,\dots,\nineq \} \ | \ y_i>0 \} \\
	\IG &:=& \{ i \in \{1,\dots,\ncomp \} \ | \ \sg_i=-\bar{t} \}\\
	\IH &:=& \{ i \in \{1,\dots,\ncomp \} \ | \ \sh_i=-\bar{t} \}\\
	\IGHOP &:=& \{ i \in \{1,\dots,\ncomp \} \ | \ \sh_i=\psi(\sg;t) \}\\
	\IGHPO &:=& \{ i \in \{1,\dots,\ncomp \} \ | \ \sg_i=\psi(\sh;t) \}\\
	\IGHOO &:=& \{ i \in \{1,\dots,\ncomp \} \ | \ \sg_{\IGHOO}=\sh_i=\psi(0;t) \}\\
	\mathcal{S}_G &:=& \{ i \in \{1,\dots,\ncomp \}\} \textbackslash (\IG \cup \IGHPO \cup \IGHOO)  \\
	\mathcal{S}_H &:=& \{ i \in \{1,\dots,\ncomp \}\} \textbackslash (\IH \cup \IGHOP \cup \IGHOO)  \\
\end{eqnarray*}
$\mathcal{Y}$, $\mathcal{S}_G$ and $\mathcal{S}_H$ denote the set of indices where the variables $\y$, $\sg$ and $\sh$ are free.

Some of the fixed variables are replaced by a constant and others are replaced by an expression that depends on free variables. It is rather clear from this observation that the use of slack variables is an essential tool to handle the non-linear bounds.
%

A major consequence here is that the gradient of $\Psi$ in this subspace must be done with care using the composition of the derivative formula :
\begin{equation}\label{eq:chain_rule}
\nabla \tilde{\Psi}_{r,\rho}(x,\s_{\mathcal{S}_G \cup \mathcal{S}_H},\y_\mathcal{Y},t) =J_{\bar{\mathcal{W}_j}}^T \nabla \Psi_{r,\rho}(x,\s,\y,t) 
\end{equation}
where $J_{\bar{\mathcal{W}_j}}$ is an $\nvar + 2\ncomp + \nineq \times \nvar + \#\mathcal{S}_G +\# \mathcal{S}_H + \# \y_\mathcal{Y}$ matrix defined such that
\begin{equation*}
J_{\bar{\mathcal{W}_j}}= \left( \begin{array}{c}
J_{\bar{\mathcal{W}_j}}^x \\
J_{\bar{\mathcal{W}_j}}^\sg \\
J_{\bar{\mathcal{W}_j}}^\sh \\
J_{\bar{\mathcal{W}_j}}^\y \\
\end{array} \right) ~.
\end{equation*}
The four sub-matrix used to define $J_{\bar{\mathcal{W}_j}}$ are computed in the following way
\begin{equation*}
J_{\bar{\mathcal{W}_j}}^x=Id_n ~,
\end{equation*}
\begin{equation*}
J_{\bar{\mathcal{W}_j},i}^\sg=\begin{cases}
e_i^T, \ i \in \mathcal{S}_G \\
\psi'(\sh_i;t)e_i^T,i \in \IGHPO\\
0, \ i \in (\{ 1,\dots,\ncomp \} \textbackslash \mathcal{S}_G) \textbackslash \IGHPO
\end{cases} ~,
\end{equation*}
\begin{equation*}
J_{\bar{\mathcal{W}_j},i}^\sh=\begin{cases}
e_i^T, \ i \in \mathcal{S}_H \\
\psi'(\sg_i;t)e_i^T,i \in \IGHOP\\
0, \ i \in (\{ 1,\dots,\ncomp \} \textbackslash \mathcal{S}_H) \textbackslash \IGHOP
\end{cases} ~,
\end{equation*}
\begin{equation*}
J_{\bar{\mathcal{W}_j},i}^\sg=\begin{cases}
e_i^T, \ i \in \mathcal{Y} \\
0,i \in \{ 1,\dots,\nineq \} \textbackslash \mathcal{Y}
\end{cases} ~,
\end{equation*}
where $J_{\bar{\mathcal{W}_j},i}$ denotes the i-th line of a matrix and $e_i$ is a vector of zero whose i-th component is one. We may proceed in a similar way to compute the hessian matrix of $\tilde{\Psi}_{r,\rho}(x,\s_{\mathcal{S}_G \cup \mathcal{S}_H},\y_\mathcal{Y},t)$.\\

The feasible direction $d^j$ is constructed to lie in a subspace defined by the working set and satisfying the sufficient-descent direction conditions :
\begin{equation}\label{eq:direction_descente_suffisante}
\begin{split}
\nabla \Psi(z^j)d^j \leq -\mu_0 \| \nabla \Psi(z^j) \|^2 \\
\|d^j \| \leq \mu_1 \| \nabla \Psi(z^j) \|
\end{split} ~,
\tag{SDD}
\end{equation}
where $\mu_0>0$, $\mu_1>0$.\\

The step length $\alpha_j \in (0,\bar{\alpha}]$ is respectively computed to satisfy the Armijo and Wolfe conditions:
\begin{equation}\label{eq:Armijo_cond}
\Psi(z^j+\alpha_jd^j) \leq \Psi(z^j)+\tau_0 \alpha_j \nabla \Psi(z^j)^Td^j, \ \tau_0 \in (0,1)
\end{equation}
\begin{equation}\label{eq:Wolfe_cond}
\nabla \Psi(z^j+\alpha_j d^j)^Td^j \geq \tau_1 \nabla \Psi(z^j)^T d^j, \ \tau_1 \in (\tau,1)
\end{equation}
If $\bar{\alpha}$ satisfies the Armijo condition \eqref{eq:Armijo_cond}, the active set strategy adds a new active constraint and the Wolfe condition \eqref{eq:Wolfe_cond} is not enforced. Otherwise, the Armijo condition requires $\alpha < \bar{\alpha}$ and the Wolfe condition is enforced.\\

The relaxing rule is given by the following scheme : Relax some constraint $i_0$ if and only if the two following conditions are fulfilled :
\begin{enumerate}
	\item $\lambda^j_{i_0}<0$;
	\item No constraint was added at the arrival point $z^j$ and no constraint was deleted at the previous iteration.
\end{enumerate}

The convergence result will rely on the fact that at least one step satisfying Wolfe's condition will be performed before removing an active constraint.\\

Convergence of this algorithm has been shown in \cite{kadrani2015globally} and is given here without proof. Indeed, the algorithm keeps the same tools. It only differs with the original one by handling the non-linear bounds.

\begin{theo}
	Suppose $f$ is bounded below on $\mathcal{F}$ and let $\{x^{k,j}\}_{j=1,\dots,\infty}$ be an infinite sequence generated by Algorithm \ref{algo:active_set_mpcc}. Then, any accumulation point $x^k$ of the sequence $\{x^{k,j}\}$ is a stationary point of \eqref{eq:penalized_nlp}.
\end{theo}

\subsection{An Algorithm for \eqref{eq:mpcc}}

\todo[inline]{En cours de r\'edaction}

We present an algorithm to solve \eqref{eq:mpcc} through a regularization-penalization scheme presented above and an active-set method.

\begin{algorithm}[H]
	\KwData{ Let $z^0=(x^0,y^0,s^0,t^0)$ an initial point such that $(y^0,s^0,t^0) \in \mathcal{F}$.\;Choose initial penalty parameters $r^0>0, \rho^0>0$. Let $k=1$. \;}
	\textbf{Begin} \;
	\Repeat{$x^*$ is M-stationary}{
		Choose a penalty parameter $r^k$ and a stopping tolerance $\epsilon^k$. Let $\epsilon^k_{relax}=\sqrt(\bar{t})$, where $\phi^k=\phi(x^k,y^k,s^k)$, and set $\rho^k=\rho^{k-1}$.\;
		\label{step:as}Active-Set Algorithm : from the starting point $z^k$, use Algorithm 1 to compute an approximate stationary point of \eqref{eq:penalized_nlp}. Algorithm 1 terminates when the iterate $z^{k_j}$ satisfies the condition ... for $\epsilon^k$. Set $\rho^k=\rho^{k_j}$.\;
		If $t_{k_j} \leq \epsilon^k_{relax}$, let $z^k=z^{k_j}$, set $k \leftarrow k+1$. Otherwise, set $\rho^k \leftarrow \sigma \rho^k$ and go to step \ref{step:as}.
	}
	\Return : $f_{opt}$ the optimal value at the solution $x_{opt}$ or a decision of infeasibility or unboundedness.
	\caption{Active-set algorithm for Problem \eqref{eq:mpcc}.}
	\label{algo:relaxation_mpcc_full}
\end{algorithm}

The following Theorem that provide convergence of the full algorithm is given as a consequence of Theorem \ref{th:convergence_butterfly_epsilon-pen}.

\begin{theo}\label{th:convergence_total}
	Let $\{ x^k,\s^k,\y^k,\m^k\}_k$ be a sequence computed by Algorithm \ref{algo:relaxation_mpcc_full} for all $k\in \bbn$ with $x^k \rightarrow x^*$ such that MPCC-CRSC holds at $x^*$.
	Then, if $x^* \in \feasmpcc$ it is an M-stationary point of \eqref{eq:mpcc}.
\end{theo}

\section{Numerics}

\todo[inline]{Un exemple ou deux de l'implantation (par exemple ceux de Kanzow et Schwarz) comme justification num\'erique.}

Example \ref{ex:mpcc_weak} continuation
\begin{exemple}
	\begin{equation*}
	\begin{array}{ll} 
	\min &x_2-x_1\\
	\textrm{s.t.}
	&0\leq x_1\perp x_2\geq0.\\
	\end{array}
	\end{equation*}
\end{exemple}

Example \ref{ex:mpcc_C_1}

\begin{exemple}
	\begin{equation*}
	\begin{array}{ll} 
	\min &\frac12((x_1-1)^2 + (x_2-1)^2)\\
	\textrm{s.t.}
	&0\leq x_1\perp x_2\geq0.\\
	\end{array}
	\end{equation*}
\end{exemple}

\section{Concluding Remarks}

\section*{Acknowledgements}
This research was partially support by a grant from "l'Ecole des Docteurs de l'UBL" and "le Conseil Régional de Bretagne". 

\bibliographystyle{plain}
\bibliography{../biblioMPEC} 

\appendix

\end{document}
